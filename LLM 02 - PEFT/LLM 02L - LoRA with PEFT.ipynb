{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b2d05a0-8b0c-4970-8a67-db5c01fb6bd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edd173be-b15c-455d-a872-224e4fbd71a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Low-Rank Adaption (LoRA)\n",
    "This lab introduces how to apply low-rank adaptation (LoRA) to your model of choice using [Parameter-Efficient Fine-Tuning (PEFT) library developed by Hugging Face](https://huggingface.co/docs/peft/index). \n",
    "\n",
    "\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "1. Apply LoRA to a model\n",
    "1. Fine-tune on your provided dataset\n",
    "1. Save your model\n",
    "1. Conduct inference using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f784443a-214a-4d6d-a528-c22644111778",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install peft==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5dee92b-1bb0-4ac3-abcf-8b98e62c9429",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUsernameFromEnv(lesson):\n",
    "  '''\n",
    "  Exception handling for when the working directory is not in the scope\n",
    "  (i.e. the Classroom-Setup was not run)\n",
    "  '''\n",
    "  try:\n",
    "    return f\"../data/testing-files/{lesson}\"\n",
    "  except NameError:\n",
    "    raise NameError(\"Working directory not found. Please re-run the Classroom-Setup at the beginning of the notebook.\")\n",
    "\n",
    "def questionPassed(userhome_for_testing, lesson, question):\n",
    "  '''\n",
    "  Helper function that writes an empty file named `PASSED` to the designated path\n",
    "  '''\n",
    "  from pathlib import Path\n",
    "\n",
    "  print(f\"\\u001b[32mPASSED\\x1b[0m: All tests passed for {lesson}, {question}\")\n",
    "\n",
    "  path = f\"{userhome_for_testing}/{question}\"\n",
    "  Path(path).mkdir(parents=True, exist_ok=True)\n",
    "  with open(f\"{path}/PASSED\", \"wb\") as handle:\n",
    "      pass # just write an empty file\n",
    "  \n",
    "  print (\"\\u001b[32mRESULTS RECORDED\\x1b[0m: Click `Submit` when all questions are completed to log the results.\")\n",
    "\n",
    "\n",
    "def dbTestQuestion1_1(new_model):\n",
    "  lesson, question = \"lesson1\", \"question1\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "  \n",
    "  assert  str(type(new_model)) == \"<class '__main__.TransformerEncoder'>\", \"Test NOT passed: Result should be of type `__main__.TransformerEncoder`\"\n",
    "  \n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "\n",
    "# LLM 02L\n",
    "\n",
    "def dbTestQuestion2_1(r, target_modules):\n",
    "  lesson, question = \"lesson2\", \"question1\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert r == 1, \"Test NOT passed: `r` should be equal to 1.\"\n",
    "  assert target_modules == [\"query_key_value\"], \"Test NOT passed: `target_modules` should be equal to `[\\\"query_key_value\\\"]`.\"\n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "def dbTestQuestion2_2(model):\n",
    "  lesson, question = \"lesson2\", \"question2\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "  \n",
    "  trainable_params = 0\n",
    "  all_param = 0\n",
    "  for _, param in peft_model.named_parameters():\n",
    "      all_param += param.numel()\n",
    "      if param.requires_grad:\n",
    "          trainable_params += param.numel()\n",
    "\n",
    "  assert trainable_params > 0, \"Test NOT passed: Your new adapted model should have more than 0 training parameters.\"\n",
    "  \n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "def dbTestQuestion2_3(trainer):\n",
    "  lesson, question = \"lesson2\", \"question3\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert str(type(trainer.args)) == \"<class 'transformers.training_args.TrainingArguments'>\", \"Test NOT passed: `trainer.args` should be of type `transformers.training_args.TrainingArguments`.\"\n",
    "\n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "def dbTestQuestion2_4(loaded_model):\n",
    "  lesson, question = \"lesson2\", \"question4\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert str(type(loaded_model)) == \"<class 'peft.peft_model.PeftModelForCausalLM'>\", \"Test NOT passed: `loaded_model` should be of class `peft.peft_model.PeftModelForCausalLM`.\"\n",
    "\n",
    "  questionPassed(userhome_for_testing, lesson, question) \n",
    "\n",
    "def dbTestQuestion2_5(outputs):\n",
    "  lesson, question = \"lesson2\", \"question5\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert str(type(outputs)) == \"<class 'torch.Tensor'>\", \"Test NOT passed: `outputs` should be of type `torch.Tensor`.\" \n",
    "\n",
    "  questionPassed(userhome_for_testing, lesson, question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "224400bc-25d5-48b6-a8c4-3eee7878648b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will re-use the same dataset and model from the demo notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e08c050-bb56-4967-9ab4-b303197a25e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"bigscience/bloomz-560m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "train_sample = data[\"train\"].select(range(50))\n",
    "display(train_sample) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03218633-bfe4-4682-8cb7-50f537d09c05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define LoRA configurations\n",
    "\n",
    "By using LoRA, you are unfreezing the attention `Weight_delta` matrix and only updating `W_a` and `W_b`. \n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/llm/lora.png\" width=300>\n",
    "\n",
    "You can treat `r` (rank) as a hyperparameter. Recall from the lecture that, LoRA can perform well with very small ranks based on [Hu et a 2021's paper](https://arxiv.org/abs/2106.09685). GPT-3's validation accuracies across tasks with ranks from 1 to 64 are quite similar. From [PyTorch Lightning's documentation](https://lightning.ai/pages/community/article/lora-llm/):\n",
    "\n",
    "> A smaller r leads to a simpler low-rank matrix, which results in fewer parameters to learn during adaptation. This can lead to faster training and potentially reduced computational requirements. However, with a smaller r, the capacity of the low-rank matrix to capture task-specific information decreases. This may result in lower adaptation quality, and the model might not perform as well on the new task compared to a higher r.\n",
    "\n",
    "Other arguments:\n",
    "- `lora_dropout`: \n",
    "  - Dropout is a regularization method that reduces overfitting by randomly and temporarily removing nodes during training. \n",
    "  - It works like this: <br>\n",
    "    * Apply to most type of layers (e.g. fully connected, convolutional, recurrent) and larger networks\n",
    "    * Temporarily and randomly remove nodes and their connections during each training cycle\n",
    "    ![](https://files.training.databricks.com/images/nn_dropout.png)\n",
    "    * See the original paper here: <a href=\"http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\" target=\"_blank\">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>\n",
    "- `target_modules`:\n",
    "  - Specifies the module names to apply to \n",
    "  - This is dependent on how the foundation model names its attention weight matrices. \n",
    "  - Typically, this can be:\n",
    "    - `query`, `q`, `q_proj` \n",
    "    - `key`, `k`, `k_proj` \n",
    "    - `value`, `v` , `v_proj` \n",
    "    - `query_key_value` \n",
    "  - The easiest way to inspect the module/layer names is to print the model, like we are doing below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86f79e2a-dc74-4a73-8073-948b6a4c141a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Fill in `r=1` and `target_modules`. \n",
    "\n",
    "Note:\n",
    "- For `r`, any number is valid. The smaller the r is, the fewer parameters there are to update during the fine-tuning process.\n",
    "\n",
    "Hint: \n",
    "- For `target_modules`, what's the name of the **first** module within each `BloomBlock`'s `self_attention`? \n",
    "\n",
    "Read more about [`LoraConfig` here](https://huggingface.co/docs/peft/conceptual_guides/lora#common-lora-parameters-in-peft)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d669007a-9a7a-43b2-8081-208a4b91fb4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "import peft\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=1,\n",
    "    lora_alpha=1, # a scaling factor that adjusts the magnitude of the weight matrix. Usually set to 1\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", # this specifies if the bias parameter should be trained. \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "276a29c6-5500-4ac7-9167-27bc9e53b30c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_1(lora_config.r, lora_config.target_modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1935b0af-a038-49ba-bd82-add663a292c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###  Question 2\n",
    "\n",
    "Add the adapter layers to the foundation model to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "735410a2-d27d-43e5-8a18-1480f249121f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"bigscience/bloomz-560m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# peft_config = PromptTuningConfig(\n",
    "#     task_type=TaskType.CAUSAL_LM,\n",
    "#     prompt_tuning_init=PromptTuningInit.RANDOM,\n",
    "#     num_virtual_tokens=4,\n",
    "#     tokenizer_name_or_path=model_name\n",
    "# )\n",
    "\n",
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c561f68-43b1-4665-9e81-4158b1f0e622",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_2(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8c84cf9-181a-4025-9f25-2cd231715c39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define `Trainer` class for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16cdfca1-baa0-49be-934c-cdcadfa2382a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 3 \n",
    "\n",
    "Fill out the `Trainer` class. Feel free to tweak the `training_args` we provided, but remember that lowering the learning rate and increasing the number of epochs will increase training time significantly. If you change none of the defaults we set below, it could take ~15 mins to fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5576cfe6-2ae3-4bd9-af62-9caefd30b55e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import os\n",
    "\n",
    "output_directory = os.path.join(\"../data\", \"peft_lab_outputs\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_directory,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate= 3e-2, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=5,\n",
    "    no_cuda=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af24982b-c7d1-4d36-b667-a1ee6e6ab385",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_3(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce008a88-7785-4f05-be12-fdec20d8f437",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3837cb29-15e1-425b-8fb4-cf22dd673a91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 4 \n",
    "\n",
    "Load the PEFT model using pre-defined LoRA configs and foundation model. We set `is_trainable=False` to avoid further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "627347b7-add5-422c-a464-5ac33b92c233",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_now = time.time()\n",
    "\n",
    "username = \"fran\" # spark.sql(\"SELECT CURRENT_USER\").first()[0]\n",
    "peft_model_path = os.path.join(output_directory, f\"peft_model_{time_now}\")\n",
    "\n",
    "trainer.model.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1898ec24-3b33-4018-ace6-1e4e7bb0d75a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "loaded_model = PeftModel.from_pretrained(peft_model, \n",
    "                                        peft_model_path, \n",
    "                                        is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bf97bb7-0c58-422a-aba2-42560a45a926",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_4(loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e0310f5-9900-464a-9cad-f8e638467be4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2062a1b4-72c2-4b0e-bc2f-eb4569150f40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Generate output tokens to the same input we provided in the demo notebook before. How do the outputs compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01f6453d-85dc-4fd6-b4bb-d3da6fe007eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "inputs = tokenizer(\"Two things are infinite: \", return_tensors=\"pt\")\n",
    "inputs = inputs.to(peft_model.device)\n",
    "outputs = peft_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"], \n",
    "    attention_mask=inputs[\"attention_mask\"], \n",
    "    max_new_tokens=28, \n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17531669-d7b2-49a7-a373-b6521d051f0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_5(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32f566c7-2bec-416c-b842-c326267c7b72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LLM 02L - LoRA with PEFT",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
